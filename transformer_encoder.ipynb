{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "b = 32 # batch size\n",
    "t = 24 # max sequence length (number of tokens)\n",
    "c = 64 # embedding dimensions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "x = torch.rand(size=(b, t, c))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, emb_dim, head_size, masked=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.masked = masked\n",
    "\n",
    "        #TODO: Check if these projections should have bias or not\n",
    "        self.toquery = nn.Linear(emb_dim, head_size)\n",
    "        self.tokey = nn.Linear(emb_dim, head_size)\n",
    "        self.tovalue = nn.Linear(emb_dim, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        # Project input into query, key, and value\n",
    "        Q = self.toquery(x) # b, t, head_size\n",
    "        K = self.tokey(x) # b, t, head_size\n",
    "        V = self.tovalue(x) # b, t, head_size\n",
    "\n",
    "        # transpose K to swap the second-to-last with the last dimension before matrix multiplication\n",
    "        att = Q @ K.transpose(-2,-1) # (b, t, head_size) @ (b, head_size, t) = b, t, t\n",
    "        att_scaled = att / (self.emb_dim ** 0.5)\n",
    "\n",
    "        # Apply masking to allow tokens to only attend to the left, not to the right\n",
    "        if self.masked:\n",
    "            mask = torch.tril(torch.ones(t, t))\n",
    "            att_scaled = att_scaled.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax scores to get weights\n",
    "        weights = F.softmax(att_scaled, dim=-1) # b, t, t\n",
    "\n",
    "        # Multiply softmaxed weights with values\n",
    "        out = weights @ V # (b, t, t) @ (b, t, head_size) = b, t, head_size\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 24, 16])"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Attention(c, int(c/4), masked=True)\n",
    "p = a(x)\n",
    "p.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, masked=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Embedding dimension must be divisble by number of heads\n",
    "        assert emb_dim % n_heads == 0\n",
    "        head_size = emb_dim // n_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(emb_dim, head_size, masked) for _ in range(n_heads)\n",
    "        ])\n",
    "\n",
    "        self.unifyheads = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through each attention head\n",
    "        out = [h(x) for h in self.heads] # n_head x [b, t, head_size]\n",
    "        # Concatenate outputs from individual heads back along the last (embedding) dimension\n",
    "        out = torch.cat(out, dim=-1) # b, t, emb_dim\n",
    "        # Pass concatenated output from all heads through linear layer\n",
    "        out = self.unifyheads(out) # # b, t, emb_dim\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 64])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(c, 4, None)\n",
    "out = mha(x)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ff_dim, n_heads, masked=False):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(emb_dim, n_heads, masked)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, emb_dim)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention\n",
    "        x = x + (self.mha(self.ln1(x)))\n",
    "        # Feed-forward\n",
    "        x = x + (self.ff(self.ln2(x)))\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 24, 64])"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb = TransformerBlock(emb_dim=c, ff_dim=c * 4, n_heads=8)\n",
    "out = tb(x)\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, ff_dim, vocab_size, max_seq_len, n_layers, n_heads, n_classes, masked=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_embeddings = nn.Embedding(max_seq_len, emb_dim)\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(emb_dim, ff_dim, n_heads, masked) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.cls_layer = nn.Linear(emb_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Retrieve token embeddings\n",
    "        embeddings = self.token_embeddings(x)\n",
    "        b, t, c = embeddings.size()\n",
    "\n",
    "        # Generate array from 0 to sequence length to retrieve positional embeddings\n",
    "        positions = torch.arange(t)\n",
    "        pos_embeddings = self.positional_embeddings(positions)\n",
    "\n",
    "        # Add token embeddings and positional encodings to obtain input into transformer blocks\n",
    "        input = embeddings + pos_embeddings\n",
    "\n",
    "        # Run data through transformer blocks\n",
    "        output = self.transformer_blocks(input)\n",
    "\n",
    "        # Take average across tokens\n",
    "        output = output.mean(dim=1) # b, t, emb_dim -> b, emb_dim\n",
    "\n",
    "        # Final linear layer to obtain one value per class\n",
    "        logits = self.cls_layer(output)\n",
    "\n",
    "        return logits\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "b = 32\n",
    "t = 24\n",
    "vocab_size = 1000\n",
    "emb_dim = 128\n",
    "ff_dim = emb_dim * 4\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "n_classes = 10\n",
    "\n",
    "tokens = torch.randint(low=0, high=vocab_size, size=(b, t))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 10])"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = TransformerEncoder(emb_dim, ff_dim, vocab_size, t, n_layers, n_heads, n_classes)\n",
    "out = te(tokens)\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "T = 8\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(tril)\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)\n",
    "sm = F.softmax(wei, dim=-1)\n",
    "print(sm)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml",
   "language": "python",
   "display_name": "Python (ML)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
