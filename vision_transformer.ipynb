{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# General transformer modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, emb_dim, head_size, masked=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.masked = masked\n",
    "\n",
    "        #TODO: Check if these projections should have bias or not\n",
    "        self.toquery = nn.Linear(emb_dim, head_size)\n",
    "        self.tokey = nn.Linear(emb_dim, head_size)\n",
    "        self.tovalue = nn.Linear(emb_dim, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        # Project input into query, key, and value\n",
    "        Q = self.toquery(x) # b, t, head_size\n",
    "        K = self.tokey(x) # b, t, head_size\n",
    "        V = self.tovalue(x) # b, t, head_size\n",
    "\n",
    "        # transpose K to swap the second-to-last with the last dimension before matrix multiplication\n",
    "        att = Q @ K.transpose(-2,-1) # (b, t, head_size) @ (b, head_size, t) = b, t, t\n",
    "        att_scaled = att / (self.emb_dim ** 0.5)\n",
    "\n",
    "        # Apply masking to allow tokens to only attend to the left, not to the right\n",
    "        if self.masked:\n",
    "            mask = torch.tril(torch.ones(t, t))\n",
    "            att_scaled = att_scaled.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax scores to get weights\n",
    "        weights = F.softmax(att_scaled, dim=-1) # b, t, t\n",
    "\n",
    "        # Multiply softmaxed weights with values\n",
    "        out = weights @ V # (b, t, t) @ (b, t, head_size) = b, t, head_size\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, masked=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Embedding dimension must be divisble by number of heads\n",
    "        assert emb_dim % n_heads == 0\n",
    "        head_size = emb_dim // n_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(emb_dim, head_size, masked) for _ in range(n_heads)\n",
    "        ])\n",
    "\n",
    "        self.unifyheads = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through each attention head\n",
    "        out = [h(x) for h in self.heads] # n_head x [b, t, head_size]\n",
    "        # Concatenate outputs from individual heads back along the last (embedding) dimension\n",
    "        out = torch.cat(out, dim=-1) # b, t, emb_dim\n",
    "        # Pass concatenated output from all heads through linear layer\n",
    "        out = self.unifyheads(out) # # b, t, emb_dim\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ff_dim, n_heads, masked=False):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(emb_dim, n_heads, masked)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, emb_dim)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention\n",
    "        x = x + (self.mha(self.ln1(x)))\n",
    "        # Feed-forward\n",
    "        x = x + (self.ff(self.ln2(x)))\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vision transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "B = 32\n",
    "C = 3\n",
    "H = 224\n",
    "W = 224\n",
    "\n",
    "P = 16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 3, 224, 224])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randint(low=0, high=255, size=(B,C,H,W), dtype=torch.float32)\n",
    "img.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 196, 768])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B, H*W/P^2, P^2*C\n",
    "img_patch = img.view((B, (H*W)//P**2, P**2 * C))\n",
    "img_patch.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, model_dim, tf_ff_dim, tf_layers, tf_heads, patch_size, max_patches, img_channels, cls_hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.embed_patches = nn.Linear(patch_size ** 2 * img_channels, model_dim)\n",
    "        self.embed_positions = nn.Embedding(max_patches, model_dim)\n",
    "        self.cls_token = nn.Parameter(data=torch.rand((1, 1, model_dim))) # batch, patch, model_dim\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(model_dim, tf_ff_dim, tf_heads) for _ in range(tf_layers)\n",
    "        ])\n",
    "\n",
    "        self.classification_head = nn.Sequential(\n",
    "            # Dosovitskiy et al. (2022) mention that at pre-training ViT uses a MLP with one hidden layer,\n",
    "            # but no further details about layer size or activation function\n",
    "            nn.Linear(model_dim, cls_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cls_hidden_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        p = self.patch_size\n",
    "\n",
    "        # Reshape image into sequence of flattened patches\n",
    "        img_patch = x.view(b, (h*w)//p**2, p**2 * c)\n",
    "\n",
    "        # Embed patches using linear projection\n",
    "        patch_embeddings = self.embed_patches(img_patch)\n",
    "\n",
    "        # Copy cls token along the batch dimension\n",
    "        cls_tokens = self.cls_token.repeat(b, 1, 1) # b, 1, model_dim\n",
    "\n",
    "        # Prepend cls tokens so that they are at position 0 of the image patches\n",
    "        patch_embeddings = torch.cat((cls_tokens, patch_embeddings), dim=1)\n",
    "        _, t, _ = patch_embeddings.size()\n",
    "\n",
    "        # Retrieve position embeddings for each patch\n",
    "        position_embeddings = self.embed_positions(torch.arange(t))\n",
    "\n",
    "        # Add patch and position embeddings to get the input into the transformer blocks\n",
    "        tf_input = patch_embeddings + position_embeddings\n",
    "\n",
    "        # Run input through transformer blocks\n",
    "        tf_output = self.transformer_blocks(tf_input)\n",
    "\n",
    "        # Pass output at cls position into classification MLP\n",
    "        out = self.classification_head(tf_output[:, 0, :])\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 10])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt = VisionTransformer(64, 64*4, 8, 4, 16, 1000, 3, 300, 10)\n",
    "out = vt(img)\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml",
   "language": "python",
   "display_name": "Python (ML)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
