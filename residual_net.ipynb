{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of ResNet-18**\n",
    "\n",
    "_Maybe later try to implement ResNet-50 with the \"bottleneck\" building blocks from the He et al. (2015) paper_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from utils import *\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data dimensions: [batch, channels, height, width)\n",
    "\n",
    "batch_size = 256\n",
    "image_channels = 3\n",
    "image_dim = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy data for testing\n",
    "\n",
    "X = torch.rand(size=(batch_size, image_channels, *image_dim))\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=64, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride,\n",
    "                               padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Batch normalisation after each convolutional layer\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "        # 1x1 convolution for residual connection\n",
    "        self.conv_skip = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=2,\n",
    "                                   padding=0)\n",
    "\n",
    "    def conv_block(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.stride == 1:\n",
    "            return self.conv_block(x) + x\n",
    "\n",
    "        # When padding in the conv block is 2 the image dimensions are halved and the number of filters doubled\n",
    "        # To match the output size of the conv block with that of the skip connection, 1x1 convolution is performed\n",
    "        # in the skip connection\n",
    "        if self.stride == 2:\n",
    "            return self.conv_block(x) + self.conv_skip(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, image_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(in_channels=image_channels, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.pool0 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # Cannot confirm if the padding values are correct, but these return correctly sized output (56x56 after the max pool)\n",
    "\n",
    "        self.conv64_1 = ResBlock(in_channels=64, out_channels=64, stride=1)\n",
    "        self.conv64_2 = ResBlock(in_channels=64, out_channels=64, stride=1)\n",
    "\n",
    "        self.conv128_1 = ResBlock(in_channels=64, out_channels=128, stride=2)\n",
    "        self.conv128_2 = ResBlock(in_channels=128, out_channels=128, stride=1)\n",
    "\n",
    "        self.conv256_1 = ResBlock(in_channels=128, out_channels=256, stride=2)\n",
    "        self.conv256_2 = ResBlock(in_channels=256, out_channels=256, stride=1)\n",
    "\n",
    "        self.conv512_1 = ResBlock(in_channels=256, out_channels=512, stride=2)\n",
    "        self.conv512_2 = ResBlock(in_channels=512, out_channels=512, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=1000)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial convolutional layer (7x7) + pooling\n",
    "        x = self.conv0(x)\n",
    "        x = self.pool0(x)\n",
    "\n",
    "        # Residual/skip connection blocks\n",
    "        x = self.conv64_1(x)\n",
    "        x = self.conv64_2(x)\n",
    "\n",
    "        x = self.conv128_1(x)\n",
    "        x = self.conv128_2(x)\n",
    "\n",
    "        x = self.conv256_1(x)\n",
    "        x = self.conv256_2(x)\n",
    "\n",
    "        x = self.conv512_1(x)\n",
    "        x = self.conv512_2(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=(2, 3))\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([256, 512])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ResNet18(image_channels=3)\n",
    "p = m(X)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [256, 64, 14, 14]           3,200\n",
      "         MaxPool2d-2            [256, 64, 7, 7]               0\n",
      "            Conv2d-3            [256, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-4            [256, 64, 7, 7]             128\n",
      "            Conv2d-5            [256, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-6            [256, 64, 7, 7]             128\n",
      "          ResBlock-7            [256, 64, 7, 7]               0\n",
      "            Conv2d-8            [256, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-9            [256, 64, 7, 7]             128\n",
      "           Conv2d-10            [256, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-11            [256, 64, 7, 7]             128\n",
      "         ResBlock-12            [256, 64, 7, 7]               0\n",
      "           Conv2d-13           [256, 128, 4, 4]          73,856\n",
      "      BatchNorm2d-14           [256, 128, 4, 4]             256\n",
      "           Conv2d-15           [256, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-16           [256, 128, 4, 4]             256\n",
      "           Conv2d-17           [256, 128, 4, 4]           8,320\n",
      "         ResBlock-18           [256, 128, 4, 4]               0\n",
      "           Conv2d-19           [256, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-20           [256, 128, 4, 4]             256\n",
      "           Conv2d-21           [256, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-22           [256, 128, 4, 4]             256\n",
      "         ResBlock-23           [256, 128, 4, 4]               0\n",
      "           Conv2d-24           [256, 256, 2, 2]         295,168\n",
      "      BatchNorm2d-25           [256, 256, 2, 2]             512\n",
      "           Conv2d-26           [256, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-27           [256, 256, 2, 2]             512\n",
      "           Conv2d-28           [256, 256, 2, 2]          33,024\n",
      "         ResBlock-29           [256, 256, 2, 2]               0\n",
      "           Conv2d-30           [256, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-31           [256, 256, 2, 2]             512\n",
      "           Conv2d-32           [256, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-33           [256, 256, 2, 2]             512\n",
      "         ResBlock-34           [256, 256, 2, 2]               0\n",
      "           Conv2d-35           [256, 512, 1, 1]       1,180,160\n",
      "      BatchNorm2d-36           [256, 512, 1, 1]           1,024\n",
      "           Conv2d-37           [256, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-38           [256, 512, 1, 1]           1,024\n",
      "           Conv2d-39           [256, 512, 1, 1]         131,584\n",
      "         ResBlock-40           [256, 512, 1, 1]               0\n",
      "           Conv2d-41           [256, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-42           [256, 512, 1, 1]           1,024\n",
      "           Conv2d-43           [256, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-44           [256, 512, 1, 1]           1,024\n",
      "         ResBlock-45           [256, 512, 1, 1]               0\n",
      "           Linear-46                [256, 1000]         513,000\n",
      "           Linear-47                  [256, 10]          10,010\n",
      "================================================================\n",
      "Total params: 11,696,130\n",
      "Trainable params: 11,696,130\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 170.85\n",
      "Params size (MB): 44.62\n",
      "Estimated Total Size (MB): 216.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(m, input_size=(1, 28, 28), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "          'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data into training and validation subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_subset, valid_subset = random_split(train_data, lengths=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (conv0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv64_1): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (conv64_2): ResBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (conv128_1): ResBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (conv128_2): ResBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (conv256_1): ResBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (conv256_2): ResBlock(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (conv512_1): ResBlock(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (conv512_2): ResBlock(\n",
       "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_skip): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18(image_channels=1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, optimiser, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimiser, factor=0.1,\n",
    "                                                       patience=2)  # patience parameter was not specified in paper\n",
    "\n",
    "results = pd.DataFrame(columns=['epoch', 'dataset', 'metric', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.4436 - Accuracy: 0.837\n",
      "        Test loss: 0.3584 - Accuracy: 0.869\n",
      "Epoch  2/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.2944 - Accuracy: 0.890\n",
      "        Test loss: 0.2926 - Accuracy: 0.892\n",
      "Epoch  3/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.2448 - Accuracy: 0.909\n",
      "        Test loss: 0.2997 - Accuracy: 0.883\n",
      "Epoch  4/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.2185 - Accuracy: 0.917\n",
      "        Test loss: 0.2908 - Accuracy: 0.894\n",
      "Epoch  5/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.1952 - Accuracy: 0.925\n",
      "        Test loss: 0.2841 - Accuracy: 0.897\n",
      "Epoch  6/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.1706 - Accuracy: 0.934\n",
      "        Test loss: 0.2945 - Accuracy: 0.899\n",
      "Epoch  7/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.1580 - Accuracy: 0.940\n",
      "        Test loss: 0.2808 - Accuracy: 0.906\n",
      "Epoch  8/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.1384 - Accuracy: 0.947\n",
      "        Test loss: 0.3299 - Accuracy: 0.887\n",
      "Epoch  9/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.1251 - Accuracy: 0.951\n",
      "        Test loss: 0.3010 - Accuracy: 0.904\n",
      "Epoch 10/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.1167 - Accuracy: 0.956\n",
      "        Test loss: 0.3024 - Accuracy: 0.901\n",
      "Epoch 11/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.1040 - Accuracy: 0.960\n",
      "        Test loss: 0.3193 - Accuracy: 0.900\n",
      "Epoch 12/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.0985 - Accuracy: 0.963\n",
      "        Test loss: 0.3166 - Accuracy: 0.908\n",
      "Epoch 13/30 - Learning rate: [0.01]\n",
      "    Training loss: 0.0854 - Accuracy: 0.967\n",
      "        Test loss: 0.3285 - Accuracy: 0.904\n",
      "Epoch 14/30 - Learning rate: [0.001]\n",
      "    Training loss: 0.0444 - Accuracy: 0.984\n",
      "        Test loss: 0.3098 - Accuracy: 0.918\n",
      "Epoch 15/30 - Learning rate: [0.001]\n",
      "    Training loss: 0.0266 - Accuracy: 0.991\n",
      "        Test loss: 0.3268 - Accuracy: 0.920\n",
      "Epoch 16/30 - Learning rate: [0.001]\n",
      "    Training loss: 0.0204 - Accuracy: 0.993\n",
      "        Test loss: 0.3474 - Accuracy: 0.919\n",
      "Epoch 17/30 - Learning rate: [0.001]\n",
      "    Training loss: 0.0163 - Accuracy: 0.995\n",
      "        Test loss: 0.3658 - Accuracy: 0.919\n",
      "Epoch 18/30 - Learning rate: [0.001]\n",
      "    Training loss: 0.0128 - Accuracy: 0.996\n",
      "        Test loss: 0.3758 - Accuracy: 0.919\n",
      "Epoch 19/30 - Learning rate: [0.001]\n",
      "    Training loss: 0.0102 - Accuracy: 0.997\n",
      "        Test loss: 0.3929 - Accuracy: 0.919\n",
      "Epoch 20/30 - Learning rate: [0.0001]\n",
      "    Training loss: 0.0078 - Accuracy: 0.998\n",
      "        Test loss: 0.3992 - Accuracy: 0.919\n",
      "Epoch 21/30 - Learning rate: [0.0001]\n",
      "    Training loss: 0.0072 - Accuracy: 0.998\n",
      "        Test loss: 0.3978 - Accuracy: 0.919\n",
      "Epoch 22/30 - Learning rate: [0.0001]\n",
      "    Training loss: 0.0068 - Accuracy: 0.999\n",
      "        Test loss: 0.4020 - Accuracy: 0.919\n",
      "Epoch 23/30 - Learning rate: [0.0001]\n",
      "    Training loss: 0.0064 - Accuracy: 0.999\n",
      "        Test loss: 0.4020 - Accuracy: 0.919\n",
      "Epoch 24/30 - Learning rate: [0.0001]\n",
      "    Training loss: 0.0065 - Accuracy: 0.999\n",
      "        Test loss: 0.4000 - Accuracy: 0.919\n",
      "Epoch 25/30 - Learning rate: [0.0001]\n",
      "    Training loss: 0.0062 - Accuracy: 0.999\n",
      "        Test loss: 0.4064 - Accuracy: 0.919\n",
      "Epoch 26/30 - Learning rate: [1e-05]\n",
      "    Training loss: 0.0062 - Accuracy: 0.999\n",
      "        Test loss: 0.4035 - Accuracy: 0.919\n",
      "Epoch 27/30 - Learning rate: [1e-05]\n",
      "    Training loss: 0.0061 - Accuracy: 0.999\n",
      "        Test loss: 0.4042 - Accuracy: 0.919\n",
      "Epoch 28/30 - Learning rate: [1e-05]\n",
      "    Training loss: 0.0061 - Accuracy: 0.999\n",
      "        Test loss: 0.4079 - Accuracy: 0.919\n",
      "Epoch 29/30 - Learning rate: [1e-05]\n",
      "    Training loss: 0.0060 - Accuracy: 0.999\n",
      "        Test loss: 0.4036 - Accuracy: 0.918\n",
      "Epoch 30/30 - Learning rate: [1e-05]\n",
      "    Training loss: 0.0060 - Accuracy: 0.999\n",
      "        Test loss: 0.4084 - Accuracy: 0.919\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_epochs):\n",
    "\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, loss_func, optimiser, device)\n",
    "    valid_loss, valid_accuracy = eval_epoch(model, valid_loader, loss_func, device)\n",
    "\n",
    "    current_lr = [p['lr'] for p in optimiser.param_groups]\n",
    "    print(f\"Epoch {e+1:>2}/{n_epochs} - Learning rate: {current_lr}\")\n",
    "    # print(f\"Epoch {e+1:>2}/{n_epochs}\")\n",
    "    print(f\"{'Training loss:':>18} {train_loss:.4f} - Accuracy: {train_accuracy:.3f}\")\n",
    "    print(f\"{'Validation loss:':>18} {valid_loss:.4f} - Accuracy: {valid_accuracy:.3f}\")\n",
    "\n",
    "    train_result = pd.DataFrame(data={'epoch': e, 'dataset': 'training', 'metric': ['loss', 'accuracy'], 'value': [train_loss, train_accuracy]})\n",
    "    valid_result = pd.DataFrame(data={'epoch': e, 'dataset': 'validation', 'metric': ['loss', 'accuracy'], 'value': [valid_loss, valid_accuracy]})\n",
    "\n",
    "    results = pd.concat([results, train_result, valid_result], axis=0)\n",
    "\n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/30 - Learning rate: [0.1]\n",
      "    Training loss: inf - Accuracy: 0.107\n",
      "        Test loss: 2.3033 - Accuracy: 0.096\n",
      "Epoch  2/30 - Learning rate: [0.1]\n",
      "    Training loss: 2.3035 - Accuracy: 0.100\n",
      "        Test loss: 2.3032 - Accuracy: 0.102\n",
      "Epoch  3/30 - Learning rate: [0.1]\n",
      "    Training loss: 2.3036 - Accuracy: 0.101\n",
      "        Test loss: 2.3033 - Accuracy: 0.096\n",
      "Epoch  4/30 - Learning rate: [0.1]\n",
      "    Training loss: 2.3035 - Accuracy: 0.100\n",
      "        Test loss: 2.3031 - Accuracy: 0.100\n",
      "Epoch  5/30 - Learning rate: [0.1]\n",
      "    Training loss: 2.3035 - Accuracy: 0.099\n",
      "        Test loss: 2.3034 - Accuracy: 0.100\n",
      "Epoch  6/30 - Learning rate: [0.1]\n",
      "    Training loss: 2.3034 - Accuracy: 0.101\n",
      "        Test loss: 2.3038 - Accuracy: 0.101\n",
      "Epoch  7/30 - Learning rate: [0.1]\n",
      "    Training loss: 2.3035 - Accuracy: 0.099\n",
      "        Test loss: 2.3034 - Accuracy: 0.101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trained_model, results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimiser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Code/Python/DeepLearning/utils.py:165\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, valid_loader, loss_func, optimiser, batch_size, epochs, device, scheduler, debug_print)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;66;03m# model = model_class()\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;66;03m# model.to(device)\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;66;03m# loss_fn = loss_func_class()\u001B[39;00m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;66;03m# optim = optimiser_class(model.parameters(), lr=start_lr)\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m--> 165\u001B[0m     train_loss, train_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m     valid_loss, valid_accuracy \u001B[38;5;241m=\u001B[39m eval_epoch(model, valid_loader, loss_func, device)\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m debug_print:\n",
      "File \u001B[0;32m~/Workspace/Code/Python/DeepLearning/utils.py:52\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, dataloader, loss_func, optimizer, device)\u001B[0m\n\u001B[1;32m     50\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Train model on batch\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m batch_loss, batch_correct, batch_total \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_loss\n\u001B[1;32m     55\u001B[0m correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_correct\n",
      "File \u001B[0;32m~/Workspace/Code/Python/DeepLearning/utils.py:36\u001B[0m, in \u001B[0;36mtrain_batch\u001B[0;34m(model, X_batch, Y_batch, loss_func, optimizer)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Calculate training accuracy\u001B[39;00m\n\u001B[1;32m     35\u001B[0m batch_pred_classes \u001B[38;5;241m=\u001B[39m batch_pred\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 36\u001B[0m correct \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mbatch_pred_classes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mY_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(Y_batch)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m batch_loss\u001B[38;5;241m.\u001B[39mitem(), correct, total\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "trained_model, results = train_model(model=model, train_loader=train_loader, valid_loader=valid_loader,\n",
    "loss_func=loss_func, optimiser=optimiser,\n",
    "                                     batch_size=batch_size,\n",
    "                                     epochs=n_epochs,\n",
    "                                     device=device, scheduler=scheduler)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/30\n",
      "    Training loss: inf - Accuracy: 0.137\n",
      "        Test loss: inf - Accuracy: 0.231\n",
      "Epoch  2/30\n",
      "    Training loss: inf - Accuracy: 0.269\n",
      "        Test loss: inf - Accuracy: 0.257\n",
      "Epoch  3/30\n",
      "    Training loss: inf - Accuracy: 0.236\n",
      "        Test loss: inf - Accuracy: 0.141\n",
      "Epoch  4/30\n",
      "    Training loss: inf - Accuracy: 0.129\n",
      "        Test loss: inf - Accuracy: 0.200\n",
      "Epoch  5/30\n",
      "    Training loss: inf - Accuracy: 0.127\n",
      "        Test loss: inf - Accuracy: 0.112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model, result \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mResNet18\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCrossEntropyLoss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdamW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_lr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Code/Python/DeepLearning/utils.py:206\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model_class, train_loader, valid_loader, loss_func_class, optimiser_class, batch_size, epochs, start_lr, device, debug_print)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;66;03m# scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.5)\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m--> 206\u001B[0m     train_loss, train_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    207\u001B[0m     valid_loss, valid_accuracy \u001B[38;5;241m=\u001B[39m eval_epoch(model, valid_loader, loss_fn, device)\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m debug_print:\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;66;03m# print(f\"Epoch {e+1:>2}/{epochs} - Learning rate: {scheduler.get_last_lr()}\")\u001B[39;00m\n",
      "File \u001B[0;32m~/Workspace/Code/Python/DeepLearning/utils.py:52\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, dataloader, loss_func, optimizer, device)\u001B[0m\n\u001B[1;32m     50\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Train model on batch\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m batch_loss, batch_correct, batch_total \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_loss\n\u001B[1;32m     55\u001B[0m correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_correct\n",
      "File \u001B[0;32m~/Workspace/Code/Python/DeepLearning/utils.py:36\u001B[0m, in \u001B[0;36mtrain_batch\u001B[0;34m(model, X_batch, Y_batch, loss_func, optimizer)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Calculate training accuracy\u001B[39;00m\n\u001B[1;32m     35\u001B[0m batch_pred_classes \u001B[38;5;241m=\u001B[39m batch_pred\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 36\u001B[0m correct \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mbatch_pred_classes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mY_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(Y_batch)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m batch_loss\u001B[38;5;241m.\u001B[39mitem(), correct, total\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model, result = train_model(model_class=ResNet18, train_loader=train_loader, valid_loader=valid_loader, loss_func_class=nn.CrossEntropyLoss, optimiser_class=torch.optim.AdamW, batch_size=batch_size, epochs=n_epochs, start_lr=lr, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the train_model function that takes the model class and creates the model inside the function, it works. With the other one that takes an initialised model as input it doesn't."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
